data(vowel.test)
head(vowel.train)
vowel.train$y = as.factor(vowel.train$y)
vowel.test$y = as.factor(vowel.test$y)
set.seed(33833)
rf <- train(y ~ .,data=vowel.train, method="rf")
summary(rf)
fm <- rf$finalModel
varImp(fm, scale=TRUE)
varImp(fm, scale=TRUE)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
head(segmentationOriginal)
training = segmentationOriginal[segmentationOriginal$Case == "Train", ]
testing = segmentationOriginal[segmentationOriginal$Case == "Test", ]
set.seed(125)
modelFit <- train(Class ~ ., data=training, method="rpart")
predictions <- predict (modelFit, newdata=testing)
print(modelFit$finalModel)
library(rattle)
fancyRpartPlot(modelFit$finalModel)
plot(modelFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modelFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234 )
head(trainSA, 1)
modelFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,
data=trainSA, method="glm", family="binomial")
summary(modelFit)
predictions <- predict (modelFit, newdata=testSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
head(testSA)
chd <- as.factor(testSA$chd)
missClass(trainSA$chd, predict (modelFit, newdata=trainSA))
missClass(testSA$chd, predict (modelFit, newdata=testSA))
#q1
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y = as.factor(vowel.train$y)
vowel.test$y = as.factor(vowel.test$y)
set.seed(33833)
fitRF <- train(y ~ .,data=vowel.train, method="rf")
fitGBM <- train(y ~ .,data=vowel.train, method="gbm")
predRf <- predict(fitRF, vowel.test)
predGBM <- predict(fitGBM, vowel.test)
confusionMatrix(predRf, vowel.test$y)$overall[1]
confusionMatrix(predGBM, vowel.test$y)$overall[1]
#subset data
pred <- data.frame(predRf, predGBM, y=vowel.test$y, agree=predRf == predGBM)
head(pred)
accuracy <- sum(predRf[pred$agree] == pred$y[pred$agree]) / sum(pred$agree)
bothPredict = rfPredict[(rfPredict == gbPredict)]
bothPredict
vowel.test.filter <-subset(vowel.test, which(rfPredict == gbPredict))
bothCM <- confusionMatrix(bothPredict, vowel.test$y)
#q2
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
# head(adData)
set.seed(62433)
fitRf <- train(diagnosis ~ ., data=training, method="rf")
fitGBM <- train(diagnosis ~ ., data=training, method="gbm")
fitLDA <- train(diagnosis ~ ., data=training, method="lda")
predRf <- predict(fitRf, testing)
predGBM <- predict(fitGBM, testing)
predLDA <- predict(fitLDA, testing)
pred <- data.frame(predRf, predGBM, predLDA, diagnosis=testing$diagnosis)
# Stack the predictions together using random forests ("rf")
fit <- train(diagnosis ~., data=pred, method="rf")
predFit <- predict(fit, testing)
c1 <- confusionMatrix(predRf, testing$diagnosis)$overall[1]
c2 <- confusionMatrix(predGBM, testing$diagnosis)$overall[1]
c3 <- confusionMatrix(predLDA, testing$diagnosis)$overall[1]
c4 <- confusionMatrix(predFit, testing$diagnosis)$overall[1]
print(paste(c1, c2, c3, c4))
# Stacked Accuracy: 0.79 is better than random forests and lda
# and the same as boosting.
# Problem 3.
set.seed(3523)
library(AppliedPredictiveModeling)
library(elasticnet)
data(concrete)
inTrain <- createDataPartition(concrete$CompressiveStrength,
p=3/4)[[1]]
training <- concrete[inTrain, ]
testing <- concrete[-inTrain, ]
set.seed(233)
fit <- train(CompressiveStrength ~ ., data=training, method="lasso")
fit
plot.enet(fit$finalModel, xvar="penalty", use.color=T) # Cement
#q4
library(lubridate)  # For year() function below
library(forecast)
dat = read.csv("q:/dev/coursera/ml/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
fit <- bats(tstrain)
fit
pred <- forecast(fit, level=95, h=dim(testing)[1])
names(data.frame(pred))
predComb <- cbind(testing, data.frame(pred))
names(testing)
names(predComb)
predComb$in95 <- (predComb$Lo.95 < predComb$visitsTumblr) &
(predComb$visitsTumblr < predComb$Hi.95)
# How many of the testing points is the true value within the
# 95% prediction interval bounds?
prop.table(table(predComb$in95))[2] # 0.9617021
# Problem 5.
set.seed(3523)
library(AppliedPredictiveModeling)
library(e1071)
data(concrete)
inTrain <- createDataPartition(concrete$CompressiveStrength, p=3/4)[[1]]
training <- concrete[inTrain, ]
testing <- concrete[-inTrain, ]
set.seed(325)
fit <- svm(CompressiveStrength ~., data=training)
# OR another way
# fit <- train(CompressiveStrength ~. data=training, method="svmRadial")
pred <- predict(fit, testing)
acc <- accuracy(pred, testing$CompressiveStrength)
acc
acc[2] # RMSE 6.715009
#q1
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y = as.factor(vowel.train$y)
vowel.test$y = as.factor(vowel.test$y)
set.seed(33833)
fitRF <- train(y ~ .,data=vowel.train, method="rf")
fitGBM <- train(y ~ .,data=vowel.train, method="gbm")
predRf <- predict(fitRF, vowel.test)
predGBM <- predict(fitGBM, vowel.test)
confusionMatrix(predRf, vowel.test$y)$overall[1]
confusionMatrix(predGBM, vowel.test$y)$overall[1]
#subset data
pred <- data.frame(predRf, predGBM, y=vowel.test$y, agree=predRf == predGBM)
head(pred)
accuracy <- sum(predRf[pred$agree] == pred$y[pred$agree]) / sum(pred$agree)
bothPredict = rfPredict[(rfPredict == gbPredict)]
bothPredict
vowel.test.filter <-subset(vowel.test, which(rfPredict == gbPredict))
bothCM <- confusionMatrix(bothPredict, vowel.test$y)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y = as.factor(vowel.train$y)
vowel.test$y = as.factor(vowel.test$y)
set.seed(33833)
fitRF <- train(y ~ .,data=vowel.train, method="rf")
fitRF
fitGBM <- train(y ~ .,data=vowel.train, method="gbm")
predRf <- predict(fitRF, vowel.test)
predGBM <- predict(fitGBM, vowel.test)
confusionMatrix(predRf, vowel.test$y)$overall[1]
confusionMatrix(predGBM, vowel.test$y)$overall[1]
pred <- data.frame(predRf, predGBM, y=vowel.test$y, agree=predRf == predGBM)
head(pred)
accuracy <- sum(predRf[pred$agree] == pred$y[pred$agree]) / sum(pred$agree)
accuracy
bothPredict = rfPredict[(rfPredict == gbPredict)]
bothPredict
vowel.test.filter <-subset(vowel.test, which(rfPredict == gbPredict))
bothCM <- confusionMatrix(bothPredict, vowel.test$y)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
fitRf <- train(diagnosis ~ ., data=training, method="rf")
fitGBM <- train(diagnosis ~ ., data=training, method="gbm")
fitLDA <- train(diagnosis ~ ., data=training, method="lda")
predRf <- predict(fitRf, testing)
predGBM <- predict(fitGBM, testing)
predLDA <- predict(fitLDA, testing)
set.seed(3523)
library(AppliedPredictiveModeling)
library(elasticnet)
data(concrete)
inTrain <- createDataPartition(concrete$CompressiveStrength,
p=3/4)[[1]]
training <- concrete[inTrain, ]
testing <- concrete[-inTrain, ]
set.seed(233)
fit <- train(CompressiveStrength ~ ., data=training, method="lasso")
fit
plot.enet(fit$finalModel, xvar="penalty", use.color=T) # Cement
version()
version
library(swirl)
swirl()
library(swirl)
swirl()
swirl("R_Programming")
swirl()
install.packages("swirl")
library(swirl)
swirl()
TRUE == TRUE
(FALSE == TRUE) == FALSE
6 == 7
6 < 7
10 <= 10
5 != 7
!(5 == 7)
TRUE & TRUE
FALSE & FALSE
TRUE & c(TRUE, FALSE, FALSE)
TRUE && c(TRUE, FALSE, FALSE)
TRUE | c(TRUE, FALSE, FALSE)
TRUE || c(TRUE, FALSE, FALSE)
5 > 8 ||
| 6 != 8 && 4 > 3.9
5 > 8 ||
6 != 8 && 4 > 3.9
isTRUE(6 > 4)
identical('twins', 'twins')
xor(5 == 6, !FALSE)
ints <- sample(10)
ints
ints > 5
which(ints > 7)
any (ints < 0)
all(ints >0)
Sys.Date()
mean(c(2, 4, 5))
mean(c(2, 4, 5))
swirl()
Sys.Date()
mean(c(2, 4, 5))
setwd("Q:/dev/coursera/dp")
library(shiny)
cat("Init");
library(ggplot2)
library(data.table)
input_file <- "kobe/data.csv"
#load data
kobe_data = read.csv(input_file,header = TRUE, skipNul = TRUE, na.strings = c('NULL', 'NA'))
#filter out blank shot_made
kobe_data_train <- subset(kobe_data, !is.na(shot_made_flag))
dim(kobe_data_train)
kobe_data_dt <- data.table(kobe_data_train)
agg_var = "opponent"
kobe_data_agg <- kobe_data_dt[, list(
sum=sum(shot_made_flag),
cnt=length(shot_made_flag),
mean=mean(shot_made_flag)
), by = c("shot_distance", "shot_type", agg_var)]
kobe_data_agg_clean <- filter(kobe_data_agg, cnt > 10)
kobe_data_agg_clean$val <- as.numeric(substr(kobe_data_agg_clean$shot_type, 0, 1)) * kobe_data_agg_clean$mean
kobe_data_agg_clean <- filter(kobe_data_agg, cnt > 10)
kobe_data_agg_clean <- subset(kobe_data_agg, cnt > 10)
dim(kobe_data_agg_clean)
kobe_data_agg_clean$val <- as.numeric(substr(kobe_data_agg_clean$shot_type, 0, 1)) * kobe_data_agg_clean$mean
p <- ggplot(
kobe_data_agg_clean,
aes_string(x = "shot_distance", y = "val", colour = agg_var, size = "cnt")
) + geom_point(shape=1) + geom_smooth()
p2 <- p + facet_wrap(as.formula(paste("~", agg_var)))
print(p2)
#aggregate
kobe_data_dt <- data.table(kobe_data_train)
agg_var = "season"
kobe_data_agg <- kobe_data_dt[, list(
sum=sum(shot_made_flag),
cnt=length(shot_made_flag),
mean=mean(shot_made_flag)
), by = c("shot_distance", "shot_type", agg_var)]
#kobe_data_agg <- summarise(by_shot_opponent, sum=sum(shot_made_flag), cnt=n(), mean=mean(shot_made_flag) )
kobe_data_agg_clean <- subset(kobe_data_agg, cnt > 10)
kobe_data_agg_clean$val <- as.numeric(substr(kobe_data_agg_clean$shot_type, 0, 1)) * kobe_data_agg_clean$mean
p <- ggplot(
kobe_data_agg_clean,
aes_string(x = "shot_distance", y = "val", colour = agg_var, size = "cnt")
) + geom_point(shape=1) + geom_smooth()
p2 <- p + facet_wrap(as.formula(paste("~", agg_var)))
print(p2)
p2 <- p + facet_wrap(as.formula(paste("~", agg_var))) + scale_y_continuous(limits=c(0, 2))
print(p2)
colnames(kobe_data_train)
agg_var = "period"
kobe_data_agg <- kobe_data_dt[, list(
sum=sum(shot_made_flag),
cnt=length(shot_made_flag),
mean=mean(shot_made_flag)
), by = c("shot_distance", "shot_type", agg_var)]
#kobe_data_agg <- summarise(by_shot_opponent, sum=sum(shot_made_flag), cnt=n(), mean=mean(shot_made_flag) )
kobe_data_agg_clean <- subset(kobe_data_agg, cnt > 10)
kobe_data_agg_clean$val <- as.numeric(substr(kobe_data_agg_clean$shot_type, 0, 1)) * kobe_data_agg_clean$mean
p <- ggplot(
kobe_data_agg_clean,
aes_string(x = "shot_distance", y = "val", colour = agg_var, size = "cnt")
) + geom_point(shape=1) + geom_smooth()
p2 <- p + facet_wrap(as.formula(paste("~", agg_var))) + scale_y_continuous(limits=c(0, 2))
print(p2)
devtools::install_github('rstudio/shinyapps')
library(shinyapps)
library(shiny)
setAccountInfo
library(shiny)
runApp('kobe')
runApp('kobe')
runApp('kobe')
runApp('kobe')
runApp('kobe')
runApp('kobe')
runApp('kobe')
runApp('kobe')
runApp('kobe')
runApp('kobe')
runApp('kobe')
runApp('kobe')
runApp('kobe')
key <- "width"
value <- 32
mylist <- list()
mylist[[ key ]] <- value
mylist[["width"]]
for (agg_var in c("action_type")) {
kobe_data_agg <- kobe_data_dt[, list(
sum=sum(shot_made_flag),
cnt=length(shot_made_flag),
mean=mean(shot_made_flag)
), by = c("shot_distance", "shot_type", agg_var)]
kobe_data_agg_clean <- subset(kobe_data_agg, cnt > 10)
kobe_data_agg_clean$val <- as.numeric(substr(kobe_data_agg_clean$shot_type, 0, 1)) * kobe_data_agg_clean$mean
kobe_data_agg_all[[agg_var]] <- kobe_data_agg_clean
}
kobe_data_agg_all <<- list()
for (agg_var in c("action_type")) {
kobe_data_agg <- kobe_data_dt[, list(
sum=sum(shot_made_flag),
cnt=length(shot_made_flag),
mean=mean(shot_made_flag)
), by = c("shot_distance", "shot_type", agg_var)]
kobe_data_agg_clean <- subset(kobe_data_agg, cnt > 10)
kobe_data_agg_clean$val <- as.numeric(substr(kobe_data_agg_clean$shot_type, 0, 1)) * kobe_data_agg_clean$mean
kobe_data_agg_all[[agg_var]] <- kobe_data_agg_clean
}
kobe_data_agg_all[["action_type"]]
kobe_data_agg_all <<- list()
for (agg_var in agg_vars) {
kobe_data_agg <- kobe_data_dt[, list(
sum=sum(shot_made_flag),
cnt=length(shot_made_flag),
mean=mean(shot_made_flag)
), by = c("shot_distance", "shot_type", agg_var)]
kobe_data_agg_clean <- subset(kobe_data_agg, cnt > 10)
kobe_data_agg_clean$val <- as.numeric(substr(kobe_data_agg_clean$shot_type, 0, 1)) * kobe_data_agg_clean$mean
kobe_data_agg_all[[agg_var]] <- kobe_data_agg_clean
}
agg_vars = c("action_type", "combined_shot_type", "period", "season", "shot_type", "shot_zone_area", "shot_zone_area", "opponent")
kobe_data_agg_all <<- list()
for (agg_var in agg_vars) {
kobe_data_agg <- kobe_data_dt[, list(
sum=sum(shot_made_flag),
cnt=length(shot_made_flag),
mean=mean(shot_made_flag)
), by = c("shot_distance", "shot_type", agg_var)]
kobe_data_agg_clean <- subset(kobe_data_agg, cnt > 10)
kobe_data_agg_clean$val <- as.numeric(substr(kobe_data_agg_clean$shot_type, 0, 1)) * kobe_data_agg_clean$mean
kobe_data_agg_all[[agg_var]] <- kobe_data_agg_clean
}
kobe_data_agg_all[["action_type"]]
kobe_data_agg_all[["period"]]
runApp('kobe')
runApp('kobe')
head(kobe_data_agg_clean)
kobe_data_dt
unique(kobe_data_dt$matchup)
grep("vs.", kobe_data_dt$matchup)
grepl("vs.", kobe_data_dt$matchup)
head(grepl("vs.", kobe_data_dt$matchup))
head(grepl("vs.", kobe_data_dt$matchup), 10)
head(kobe_data_dt$matchup)
head(kobe_data_dt$matchup, 10)
kobe_data_dt$home_away <- grepl("vs.", kobe_data_dt$matchup)
head(kobe_data_dt, 10)
runApp('kobe')
runApp('kobe')
runApp('kobe')
runApp('kobe')
runApp('kobe')
runApp('kobe')
runApp('kobe')
input_file <- "kobe/data.csv"
#load data
kobe_data = read.csv(input_file,header = TRUE, skipNul = TRUE, na.strings = c('NULL', 'NA'))
#filter out blank shot_made
kobe_data_train <- subset(kobe_data, !is.na(shot_made_flag))
#aggregate
kobe_data_dt <- data.table(kobe_data_train)
summary(kobe_data_dt)
runApp('kobe')
kobe_data_dt$final_shot <- with(kobe_data_dt, minutes_remaining == 0 & seconds_remaining <=24)
kobe_data_dt$final_shot
head(kobe_data_dt$final_shot, 10)
head(kobe_data_dt$final_shot, 100)
head(kobe_data_dt$final_shot, 25)
head(kobe_data_dt$final_shot, 30)
kobe_data_dt[30,]
runApp('kobe')
kobe_data_agg_all[["shot_type"]]
kobe_data_agg_all
summary(kobe_data_dt)
runApp('kobe')
runApp('kobe')
runApp('kobe')
runApp('kobe')
require(devtools)
install_github("slidify", "nickkz")
install_github("slidify", "ramnathv")
install_github("slidifyLibraries", "ramnathv")
library(slidify)
author("mydeck")
slidify("index.Rmd")
publish(user = "nickkz", repo = "nk_jhu_dp")
publish(user = "nickkz", repo = "nk_jhu_dp2")
publish(user = "nickkz", repo = "nk_jhu_dp2")
slidify("index.Rmd")
setwd("Q:/dev/coursera/dp/kobe")
library(slidify)
author("ny_jhu_dp_kobe")
runApp('Q:/dev/coursera/dp/kobe/R')
author("nk_jhu_dp_kobe")
runApp('Q:/dev/coursera/dp/kobe/R')
library(slidify)
author("nk_jhu_dp_kobe")
slidify("index.Rmd")
runApp('Q:/dev/coursera/dp/kobe/R')
#init
cat("Server Start");
#libraries
library(shiny)
library(ggplot2)
library(data.table)
#globals
setwd("Q:/dev/coursera/dp/kobe")
input_file <- "data/data.csv"
#load data
kobe_data <- read.csv(input_file,header = TRUE, skipNul = TRUE, na.strings = c('NULL', 'NA'))
#filter out blank shot_made
kobe_data_train <- subset(kobe_data, !is.na(shot_made_flag))
#transform into data table
kobe_data_dt <- data.table(kobe_data_train)
#add home_away flag
kobe_data_dt$home_away <- grepl("vs.", kobe_data_dt$matchup)
kobe_data_dt$final_shot <- with(kobe_data_dt, minutes_remaining == 0 & seconds_remaining <= 12)
kobe_data_agg_all <<- list()
agg_vars
head(kobe_data_dt, 1)
table(kobe_data_dt$shot_type)
table(kobe_data_dt$shot_type * shot_made_flag)
table(kobe_data_dt$shot_type * kobe_data_dt$shot_made_flag)
kobe_data_agg <- kobe_data_dt[, list(
sum=sum(shot_made_flag),
cnt=length(shot_made_flag),
mean=mean(shot_made_flag)
), by = c("shot_distance", "shot_type", agg_var)]
kobe_data_agg <- kobe_data_dt[, list(
sum=sum(shot_made_flag),
cnt=length(shot_made_flag),
mean=mean(shot_made_flag)
), by = c("shot_distance", "shot_type")]
kobe_data_agg
kobe_data_agg <- kobe_data_dt[, list(
sum=sum(shot_made_flag),
cnt=length(shot_made_flag),
mean=mean(shot_made_flag)
), by = c("shot_type")]
kobe_data_agg
library(data.table)
#globals
setwd("Q:/dev/coursera/dp/kobe")
input_file <- "data/data.csv"
#load data
kobe_data <- read.csv(input_file,header = TRUE, skipNul = TRUE, na.strings = c('NULL', 'NA'))
#filter out blank shot_made
kobe_data_train <- subset(kobe_data, !is.na(shot_made_flag))
#transform into data table
kobe_data_dt <- data.table(kobe_data_train)
#aggregate
kobe_data_agg <- kobe_data_dt[, list(
made_shots=sum(shot_made_flag),
total_shots=length(shot_made_flag),
made_pct=mean(shot_made_flag)
), by = c("shot_type")]
kobe_data_agg
kobe_data_agg$shot_val <- as.numeric(substr(kobe_data_agg$shot_type, 0, 1)) * kobe_data_agg$made_pct
kobe_data_agg
